---
# title: "Improving Computer Vision Accuracy using Convolutions"
output:
  html_document:
    df_print: paged
    theme: cerulean
    highlight: breezedark
    toc: TRUE
    toc_float: TRUE
    code_download: TRUE
---


# **Improving Computer Vision Accuracy using Convolutions:** An R Version

Hello! Welcome to the fourth (includes the 3rd) **R** code walkthrough of the session ***Machine Learning Foundations*** where the awesome [Laurence Moroney](https://www.linkedin.com/in/laurence-moroney),a Developer Advocate at Google working on Artificial Intelligence, takes us through the fundamentals of building machine learning models using TensorFlow.


In this episode, [Episode 4](https://www.youtube.com/watch?v=dd8H4fiL9Yc&t=1s), Laurence Moroney takes us through yet another exciting application of Machine Learning.
Here, we put convolutions and pooling to use and create our own convolutional neural networks! This makes the Fashion MNIST neural network more efficient -- because it will classify based on features, and not on raw pixels. Convolutional layers learn the features and pass these to the dense layers which map the learned features to the given labels.

Like the previous [R Notebooks](rpubs.eR_ic), this Notebook tries to replicate the [Python Notebook](https://colab.research.google.com/github/lmoroney/mlday-tokyo/blob/master/Lab4-Using-Convolutions.ipynb) used for this episode.


Before we begin, I highly recommend that you follow [Episode 3](https://www.youtube.com/watch?v=PCgLmzkRM38&t=1) and [Episode 4](https://www.youtube.com/watch?v=dd8H4fiL9Yc&t=1s) where Laurence Moroney demystifies the concepts of **convolution** and **pooling** in computer vision. I will try and highlight some of the stuff Laurence Moroney said and add some of my own for the sake of completeness of this post but I highly recommend you listen from him first.


# **A high level overview of Convolution and Pooling**

A `convolution` is a filter that passes over an image, processing it, and extracting features that show a commonolatity in the image such that if an image has certain features, it belongs to a particular class. At its heart, convolution is really simple. It involves scanning every pixel in the image, looking at it's neighboring pixels, multiplying these pixels by their corresponding weight in a filter and then summing this all up to obtain a new pixel value. This can be shown as below:

```{r,echo=FALSE}
library(knitr)
include_graphics("C:/Users/ADMIN/Desktop/Intoduction to Python for data science/R for data science/aRduino/r4g/convolution.JPG")
print("Image source: https://colab.research.google.com/github/lmoroney/mlday-tokyo/blob/master/Lab3-What-Are-Convolutions.ipynb#scrollTo=xF0FPplsgHNh", quote = F)

```



The previous `Dense Neural Network` that we created simply learned from the raw pixels what made up a sweater or what made up a boot. This in itself is quite a limitation. 

Ultimately the goal of trying to understand what an item is, isn't just matching the raw pixels to labels like we did in the previous exercises.

What if we could extract features from the image instead and when an image has some specific features, it belongs to a particular class. This is the heart of Convolution Neural Networks do.

This key characteristic gives convnets two interesting properties:

* *The patterns they learn are translation-invariant:*
This means that after learning a certain pattern, a covnet can recognize it anywhere else in the image as opposed to a DNN which would have to learn the pattern a new if it appeared at a new location. For this reason, covnets require few training samples.

* *They can learn spatial hierarchies of patterns:*
A first convolution layer will learn small local patterns such as edges, a second convolution layer will learn larger patterns made of the features of the first layers, and so on. For this reason, covnets can learn increasingly complex and abstract features.


If you've ever done image processing using a filter (like this: https://en.wikipedia.org/wiki/Kernel_(image_processing)) then convolutions will look very familiar.

In short, you take an array (usually 3x3 or 5x5) and pass it over the image. By changing the underlying pixels based on the formula within that matrix, you can do things like edge detection. So, for example, if you look at the above link, you'll see a 3x3 that is defined for edge detection where the middle cell is 8, and all of its neighbors are -1. In this case, for each pixel, you would multiply its value by 8, then subtract the value of each neighbor. Do this for every pixel, and you'll end up with a new image that has the edges enhanced.



*That's the concept of Convolutional Neural Networks. Add some layers to do convolution before you have the dense layers, and then the information going to the dense layers is more focussed, and possibly more accurate.*


`Pooling` reduces the amount of irrelevant information in an image while maintaining the features that are detected. It does so by looking at a pixel and its immediate neighbours to the right, beneath and right-beneath, takes the largest hence the name Max pooling, and loads it into a new image. It thus reduces the amount of information that a model has to process while still maintaining the prominent features.

```{r,echo=FALSE}
library(knitr)
include_graphics("C:/Users/ADMIN/Desktop/Intoduction to Python for data science/R for data science/aRduino/r4g/pooling.JPG")
print("Image source: https://colab.research.google.com/github/lmoroney/mlday-tokyo/blob/master/Lab3-What-Are-Convolutions.ipynb#scrollTo=xF0FPplsgHNh", quote = F)

```



### **Building Convolution Neural Networks.**


# **Gathering the Data**

Let's start by loading the libraries required for this session.

We'll be requiring some packages in the Tidyverse and Keras(a framework for defining a neural network as a set of Sequential layers). You can have them installed as follows

```
suppressMessages(install.packages("tidyverse"))
suppressMessages(install.packages("keras"))
suppressMessages(install_keras())

Ps: it could take a while
```
Once installed, let's get rolling:

```{r}
library(keras)

mnist <- dataset_fashion_mnist()

c(training_images, training_labels) %<-% mnist$train
c(test_images, test_labels) %<-% mnist$test

# the train_images and train_labels arrays are the training set
# (the data the model uses to learn). 


# The model is tested against the test set: 
# the test_images, and test_labels arrays.
```


**Reshaping our image arrays and normalizing them**


```{r}
training_images <- array_reshape(training_images, c(60000, 28, 28, 1))

training_images <- training_images/255

test_images <- array_reshape(test_images, c(10000, 28, 28, 1))
test_images <- test_images/255

# the values 60,000 and 10,000 are not arbitrary, we obtained them using
dim(training_images)

# 60,000 images in the training set,
# with each image represented as 28 x 28 pixels and 1 depth of 1(binary image)
```

Why are we adding one more dimension? That's an important question.
Convolutions operate over 3D tensors, called feature maps, with two spatial axes (height width) as well as a depth axis. For an RGB depth channels image, the dimension of the depth axis is 3, because the image has 3 color channels: red, green, and blue. For a black-and-white picture, like the Fashion MNIST dataset, the depth is 1.




# **Instantiating a Convolution**

Next is to define your model. Now instead of the input layer at the top, we're going to add a Convolution. The parameters are:

1. The number of convolutions you want to generate. Purely arbitrary, but good to start with something in the order of 32

2. The size of the Convolution, in this case a 3x3 grid

3. The activation function to use -- in this case we'll use relu, which you might recall is the equivalent of returning x when x>0, else returning 0

We'll follow the Convolution with a MaxPooling layer which is then designed to compress the image, while maintaining the content of the features that were highlighted by the convlution. By specifying (2,2) for the MaxPooling, the effect is to quarter the size of the image. Without going into too much detail here, the idea is that it creates a 2x2 array of pixels, and picks the biggest one, thus turning 4 pixels into 1. It repeats this across the image, and in so doing halves the number of horizontal, and halves the number of vertical pixels, effectively reducing the image by 25%. These concepts are clearly explained in [Episode 3](https://www.youtube.com/watch?v=PCgLmzkRM38&t=1).


```{r}
model <- keras_model_sequential() %>% 
  # adding the first convolution layer with 64 3by3 filter
  # we add a color depth of 1 since convolutions operate over 3D tensors
  layer_conv_2d(input_shape = c(28, 28, 1), filters = 64, kernel_size = c(3,3), activation = 'relu') %>% 
  # adding a max pooling layer which halves the dimensions
  layer_max_pooling_2d(pool_size = c(2,2)) %>% 
  # adding a second convolution layer which filters the results
  # from the previous layer
  layer_conv_2d(filters = 64, kernel_size = c(3,3), activation = 'relu') %>% # adding a pooling layer
  layer_max_pooling_2d(pool_size = c(2,2))
  
```




# **Adding a classifier to the covnet**
Convolutional layers learn the features and pass these to the dense layers which map the learned features to the given labels. Therefore, the next step is to feed the last output tensor into a densely
connected classifier network like those we‚Äôre already familiar with: a stack of dense layers.
These classifiers process vectors, which are 1D, whereas the current output is a 3D tensor. First we have to flatten the 3D outputs to 1D, and then add a few dense layers on top.

```{r}
model <- model %>% 
  # Now flatten the output.
  layer_flatten() %>% 
  # After this you'll just have the same DNN structure as the non convolutional version
  layer_dense(units = 128, activation = 'relu') %>% 
  layer_dense(units = 10, activation = 'softmax')

```

**Compile:** Configuring a Keras model for training

```{r}
model %>% compile(
  loss = 'sparse_categorical_crossentropy',
  optimizer = optimizer_adam(),
  metrics = c('accuracy')

)
model %>% summary()
```
From the summary, we expect output of the first convolution to be a 28x28 but we obtain a 26x26. If you have watched the episode, you probably know why. A 3 by 3 filter requiring a neighbour on all sides can't work on the pixels around the edges of the picture. You effectively have to remove one pixel from the top,bottom left and rignt and this reduces your dimension by 2 on each axis. So a 28 by 28 becomes a 26 by 26.
Also, the pooling layer clearly halves the dimensions of each axis.



# **Training the Neural Network**

This is the process of training the neural network, where it 'learns' the relationship between the train_images and train_labels arrays.

To start training, call the fit method ‚Äî the model is ‚Äúfit‚Äù to the training data for a fixed number of epochs.

```{r}
history <- model %>% 
  fit(
    x = training_images,
    y = training_labels,
    epochs = 5
  )

history
```




# **Evaluating the model**

This is the step where we evaluate how accurately the network learnt to classify the images using the test_set

```{r}
metrics <- model %>% evaluate(x = test_images, y = test_labels)
cat("test_loss:", metrics$loss, "\n")
cat("test_accuracy", metrics$acc, "\n")
```


It's likely gone up to about 93% on the training data and 91% on the validation data.

That's significant, and a step in the right direction!

Try running it for more epochs -- say about 20, and explore the results! But while the results might seem really good, the validation results may actually go down, due to something called 'overfitting' which will be discussed later.
Overfitting occurs when a model is trained so intensely on training data that it becomes so familiar to the particular dataset to an extent that it fails to predict new data correctly.




# **Visualizing every filter output in each convolution and pooling layer**

For this step, I took a little detour from what was done in the Python Notebook but it illustrates the same concept: how the convolutions apply different filters to extract features from our input images.

```{r}
# let's visualize the filters applied by the convolutions using our
# first image

img_tensor <- test_images[1, , ,]

# let's view our first image
plot(as.raster(img_tensor))

# extracting the outputs of the top 4 layers
layer_output <- lapply(model$layers[1:4], function(layer) layer$output)


# Creates a model that will return these outputs, given the model input
# This model has one input and four outputs:
# one output per layer activation.
activation_model <- keras_model(inputs = model$input, outputs = layer_output)


# using the model to predict the output of the filters in the repective layers
# we still have to reshape our img_tensor since convolutions work with
# 3D inputs

activations <- activation_model %>% predict(array_reshape(img_tensor, c(1, 28, 28, 1)))

```

**let's extract the first 4 convolution and pooling layers**

```{r}
first_layer_activation <- activations[[1]]
second_layer_activation <- activations[[2]]
third_layer_activation <- activations[[3]]
fourth_layer_activation <- activations[[4]]


dim(first_layer_activation)
# the first layer has 64 filters as we defined earlier
```


Next we define a function that will help us visualise the result of each filter in each of the layer activations above.

```{r}
plot_channel <- function(channel) {
  # rotating the images
  img <- t(apply(channel, 2, rev))
  image(img, axes = FALSE, asp=1, col = hcl.colors(33))
}
```


**Visualizing the convolutions and pooling on our first test image**

The output of each filter in each layer is as shown row-wise.
```{r}
op <- par(mfrow = c(8,8), mai = c(0.05, 0, 0, 0))
for (i in 1:dim(first_layer_activation)[4]) {
 plot_channel(first_layer_activation[1,,,i])
 paste0("first layer")
}
```


```{r}
op <- par(mfrow = c(8,8), mai = c(0.05, 0, 0, 0))
for (i in 1:dim(second_layer_activation)[4]) {
 plot_channel(second_layer_activation[1,,,i])
 paste0("second layer")
}
```


```{r}
op <- par(mfrow = c(8,8), mai = c(0.05, 0, 0, 0))
for (i in 1:dim(third_layer_activation)[4]) {
 plot_channel(third_layer_activation[1,,,i])
 paste0("third layer")
}
```

```{r}
op <- par(mfrow = c(8,8), mai = c(0.05, 0, 0, 0))
for (i in 1:dim(fourth_layer_activation)[4]) {
 plot_channel(fourth_layer_activation[1,,,i])
 paste0("fourth layer")
}
```


Great! Now there are a few things to note:

* The first layer acts as a collection of various filters. At that stage, the output of the layer seems to retain almost all of the information present in the initial picture.

* As you go higher, the outputs of the layers become increasingly abstract and less visually interpretable. They begin to encode higher-level concepts such as the "heel" and "vamp" of the shoe.
Higher presentations carry increasingly less information about the visual contents of the image, and increasingly more information related to the class of the image.





# **Exercises**

The following exercises are from the Python Notebook used for this session.

1. Try editing the convolutions. Change the 32s to either 16 or 64. What impact will this have on accuracy and/or training time.

2. Remove the final Convolution. What impact will this have on accuracy or training time?

3. How about adding more Convolutions? What impact do you think this will have? Experiment with it.

4. Remove all Convolutions but the first. What impact do you think this will have? Experiment with it.

5. In the previous lesson you implemented a callback to check on the loss function and to cancel training once it hit a certain amount. See if you can implement that here!

I will attempt the last question on using callbacks to stop the training process.

Keras includes a number of [built-in callbacks](https://keras.rstudio.com/articles/training_callbacks.html#built-in-callbacks). 
For this exercise, we will [build our own callback](https://keras.rstudio.com/articles/training_callbacks.html#custom-callbacks) which stops the model from training once a desired accuracy is attained say 90%. Training should stop after the end of the 3rd epoch when accuracy goes above 90%.

**Custom Callbacks**

You can create a custom callback by creating a new R6 class that inherits from the KerasCallback class.

```{r eval=FALSE, include= TRUE}
library(R6)

# define custom callback class
train_stop <- R6::R6Class("train_stop",
 inherit = KerasCallback,
 public = list(
   on_epoch_end = function(epoch, logs = list()){
     if(logs$'acc'>0.9){
       self$model$stop_training = TRUE
       paste0("Reached 90% accuracy so cancelling training!")
     }
   }
                            
                          ))
```


That done, we just have to create an instance of the callback and Attach the callback to model training as below:

```{r eval=FALSE, include= TRUE}
# creates an instance of the callback
callback <- train_stop$new()

# loading the required packages
library(keras)

# importing the datasets directly from Keras
mnist <- dataset_fashion_mnist()
c(training_images, training_labels) %<-% mnist$train
c(test_images, test_labels) %<-% mnist$test

# Reshaping our image arrays and normalizing them
training_images <- array_reshape(training_images, c(60000, 28, 28, 1))
training_images <- training_images/255
test_images <- array_reshape(test_images, c(10000, 28, 28, 1))
test_images <- test_images/255

# Instantiating a Convolution to extract features
model <- keras_model_sequential() %>%
  layer_conv_2d(input_shape = c(28, 28, 1), filters = 64, kernel_size = c(3,3), activation = 'relu') %>% 
  layer_max_pooling_2d(pool_size = c(2,2)) %>% 
  layer_conv_2d(filters = 64, kernel_size = c(3,3), activation = 'relu') %>% layer_max_pooling_2d(pool_size = c(2,2))

# Adding a classifier to the covnet which maps the learned features to the given labels
model <- model %>%
  layer_flatten() %>%
  layer_dense(units = 128, activation = 'relu') %>%
  layer_dense(units = 10, activation = 'softmax')

#  Configuring a Keras model for training
model %>% compile(
  loss = 'sparse_categorical_crossentropy',
  metrics = c('accuracy'),
  optimizer = optimizer_adam()
)

# Training the NN and attaching a callback
history <- model %>% 
  fit(x = training_images,
      y = training_labels,
      epochs = 5,
       # Attach the callback to model training
      callbacks = list(callback)
  
)
history

# Evaluating the model
metrics <- model %>% evaluate(
  x = test_images,
  y = test_labels
)
cat("Test loss", metrics$loss, "\n")
cat("Test accuracy", metrics$acc, "\n")
```


By now, you know the drill. Laurence leaves quite a neat exercise at the end of each episode which helps one implement the concept learnt here to a new dataset. The exercise for this episode can be found here: [Exercise 3](https://colab.research.google.com/github/lmoroney/dlaicourse/blob/master/Exercises/Exercise%203%20-%20Convolutions/Exercise%203%20-%20Question.ipynb#scrollTo=sfQRyaJWAIdg). Its solution will be given in the next episode but I bet it will be something close to this. 


```{r eval=FALSE, include=TRUE}
# my solution
# improve MNIST to 99.8% accuracy
# using only a single convolutional layer and a single MaxPooling 2D. 
# You should stop training once the accuracy goes above this amount.
# It should happen in less than 20 epochs.
# When 99.8% accuracy has been hit print out the string "Reached 99.8% accuracy so cancelling training!"




# ======================================================================
library(R6)

# define custom callback class
train_stop <- R6::R6Class("train_stop",
 inherit = KerasCallback,
 public = list(
   on_epoch_end = function(epoch, logs = list()){
     if(logs$'acc'>0.998){
       self$model$stop_training = TRUE
       paste0("Reached 99.8% accuracy so cancelling training!")
     }
   }
                            
  ))

# ======================================================================


  
  
# loading the required packages
library(keras) 
  
# creates an instance of the callback
callback <- train_stop$new()



# importing the datasets directly from Keras
mnist <- dataset_mnist()
c(training_images, training_labels) %<-% mnist$train
c(test_images, test_labels) %<-% mnist$test

# Reshaping our image arrays and normalizing them
training_images <- array_reshape(training_images, c(60000, 28, 28, 1))
training_images <- training_images/255
test_images <- array_reshape(test_images, c(10000, 28, 28, 1))
test_images <- test_images/255

# Instantiating a Convolution to extract features
model <- keras_model_sequential() %>%
  layer_conv_2d(input_shape = c(28, 28, 1), filters = 128, kernel_size = c(3,3), activation = 'relu') %>% 
  layer_max_pooling_2d(pool_size = c(2,2))  
  

# Adding a classifier to the covnet which maps the learned features to the given labels
model <- model %>%
  layer_flatten() %>%
  layer_dense(units = 128, activation = 'relu') %>%
  layer_dense(units = 10, activation = 'softmax')

#  Configuring a Keras model for training
model %>% compile(
  loss = 'sparse_categorical_crossentropy',
  metrics = c('accuracy'),
  optimizer = optimizer_adam()
)

# Training the NN and attaching a callback
history <- model %>% 
  fit(x = training_images,
      y = training_labels,
      epochs = 10,
       # Attach the callback to model training
      callbacks = list(callback)
  
)
history

# Evaluating the model
metrics <- model %>% evaluate(
  x = test_images,
  y = test_labels
)
cat("Test loss", metrics$loss, "\n")
cat("Test accuracy", metrics$acc, "\n")

```

Yes! We dit it!! ü§© We successfully built a model which implements convolutions and pooling to improve the accuracy of computer vision using R. Indeed R, at its core is a beautiful and elegant language, well designed for Data Science üíñ.


# **Reference Material**

* Machine Learning Foundations: Ep #4 - [Coding with Convolutional Neural Networks](https://www.youtube.com/watch?v=dd8H4fiL9Yc&t=1s)

* Machine Learning Foundations: Ep #3 - [Convolutions and pooling](https://www.youtube.com/watch?v=PCgLmzkRM38&t=1)

* Deep Learning with R by Francois Chollet and J.J.Allaire

* The [R interface to Keras](https://keras.rstudio.com) website

* Lab 4- [Using Convolutions](https://colab.research.google.com/github/lmoroney/mlday-tokyo/blob/master/Lab4-Using-Convolutions.ipynb#scrollTo=zldEXSsF8Noz) 

* Lab 3- [What are Convolutions](https://colab.research.google.com/github/lmoroney/mlday-tokyo/blob/master/Lab4-Using-Convolutions.ipynb#scrollTo=zldEXSsF8Noz) 

* Exercise for this episode: [Exercise 3](https://colab.research.google.com/github/lmoroney/dlaicourse/blob/master/Exercises/Exercise%203%20-%20Convolutions/Exercise%203%20-%20Question.ipynb#scrollTo=sfQRyaJWAIdg)

```{r eval=FALSE, include=FALSE}
# This saves the output of the convolutions and pooling in a folder
# within the current directory
dir.create("boot_activations")
image_size <- 58
images_per_row <- 16
for (i in 1:4) {
  
  layer_activation <- activations[[i]]
  layer_name <- model$layers[[i]]$name
 
  n_features <- dim(layer_activation)[[4]]
  n_cols <- n_features %/% images_per_row
 
  png(paste0("boot_activations/", i, "_", layer_name, ".png"), 
      width = image_size * images_per_row, 
      height = image_size * n_cols)
  op <- par(mfrow = c(n_cols, images_per_row), mai = rep_len(0.02, 4), new = TRUE)
  
  for (col in 0:(n_cols-1)) {
    for (row in 0:(images_per_row-1)) {
      channel_image <- layer_activation[1,,,(col*images_per_row) + row + 1]
      plot_channel(channel_image)
    }
  }
  
  par(op)
  dev.off()
}
```

